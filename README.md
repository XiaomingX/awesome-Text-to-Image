# 文本到图像（T2I）生成与编辑：2024精选资源手册
本手册整理了**文本到图像（Text-to-Image, T2I）生成与编辑领域的核心资源**，聚焦2023-2024年前沿进展，剔除过时内容，补充模型核心价值与实用解读，帮你快速理解领域重点。


## 一、什么是文本到图像生成？
文本到图像生成是**计算机视觉（CV）与自然语言处理（NLP）交叉的核心技术**：通过深度学习模型（目前主流是“扩散模型”，可理解为“逐步去噪生成清晰图像”的技术），将文字描述（关键词、句子甚至段落）转化为符合语义的真实或创意图像。

近年该领域爆发式发展，核心价值在于**降低视觉创作门槛**——无需专业绘画技能，仅用文字即可生成设计稿、艺术创作、场景示意图等内容。


## 二、2023-2024核心生成模型（附实用解读）
以下模型按“通用基础模型”“专项任务模型”分类，优先收录2024年重磅成果，标注核心优势与可用资源（论文/代码/演示）。

### 1. 通用基础模型（全能型，覆盖各类场景）
| 模型名称               | 发布信息（会议/机构） | 核心优势（通俗解读）                                                                 | 资源链接                                                                 |
|------------------------|----------------------|--------------------------------------------------------------------------------------|--------------------------------------------------------------------------|
| **Imagen 3**           | 2024, Google         | 谷歌旗舰模型，**画质精细度、语义与图像匹配度顶尖**，支持多轮编辑和高分辨率输出。     | [论文](https://arxiv.org/abs/2408.07009)、[官方主页](https://imagen.research.google/) |
| **Kandinsky 3**        | 2024, AI Forever     | 开源全能框架，**支持多模态输入（文本+图像）**，生成风格多样，对中文提示兼容性较好。  | [论文](https://arxiv.org/abs/2410.21061)、[代码](https://github.com/ai-forever/Kandinsky-3)、[演示](https://ai-forever.github.io/Kandinsky-3/) |
| **SDXL-Lightning**     | 2024, ByteDance      | Stable Diffusion（SD）系列提速版，**1-4步即可生成高清图**（传统模型需50+步），效率极高。 | [论文](https://arxiv.org/abs/2402.13929)、[HuggingFace](https://huggingface.co/ByteDance/SDXL-Lightning)、[演示](https://fastsdxl.ai/) |
| **Ranni**              | 2024, Ali-ViLab      | **精准跟随复杂指令**（比如“穿红色大衣、戴黑色围巾的猫坐在木质书架上”），细节还原强。 | [论文](https://arxiv.org/abs/2311.17002)、[代码](https://github.com/ali-vilab/Ranni) |
| **Kandinsky 2**        | 2023, AI Forever     | 经典开源模型，虽为2023年作品，但**社区支持完善**，适合入门学习和二次开发。           | [论文](https://arxiv.org/abs/2310.03502)、[代码](https://github.com/ai-forever/Kandinsky-2) |


### 2. 专项任务模型（聚焦细分场景）
#### （1）面部/人像生成
- **PreciseControl**（ECCV 2024, Rishubh Parihar等）：解决“面部细节难控制”问题，比如精准调整“眉毛形状、瞳孔颜色”等细粒度属性，对人像创作极其实用。[论文](https://www.arxiv.org/abs/2408.05083)、[项目](https://rishubhpar.github.io/PreciseControl.home/)
- **CosmicMan**（CVPR 2024, Shikai Li等）：专注人物生成的基础模型，支持不同姿态、服饰、背景的人像合成，生成人物比例更自然。[论文](https://arxiv.org/abs/2404.01294)、[项目](https://cosmicman-cvpr2024.github.io/)
- **插入任何人**（NeurIPS 2023, Ge Yuan等）：输入一张人脸照片，即可让模型“记住”该人脸，后续生成包含此人物的各类场景图像（如“张三在月球上散步”）。[论文](https://arxiv.org/abs/2306.00926)、[项目](https://celeb-basis.github.io/)

#### （2）3D/动态生成延伸
- **快速文本到3D面部生成**（ICML 2024, Jinlu Zhang等）：从文字直接生成可编辑的3D人脸模型，支持旋转、表情调整，适用于游戏、元宇宙场景。[论文](https://arxiv.org/abs/2403.06702)、[代码](https://github.com/Aria-Zhangjl/E3-FaceNet)
- **PlacidDreamer**（ACMMM 2024, Shuo Huang等）：开源文本到3D模型，生成速度比同类模型快30%，且物体纹理更清晰（如“一个带花纹的陶瓷花瓶”）。[论文](https://arxiv.org/abs/2407.13976)、[代码](https://github.com/HansenHuang0823/PlacidDreamer)
- **Sora**（2024, OpenAI）：文本到视频旗舰模型（延伸方向），可生成1分钟以上高清视频，支持复杂场景和运动逻辑（如“一个人在雪山滑雪，镜头跟随旋转”）。[主页](https://openai.com/sora)、[技术报告](https://openai.com/research/video-generation-models-as-world-simulators)


## 三、关键技术方向（怎么让生成更“好用”？）
T2I的核心挑战是“可控性、效率、多样性”，以下是2024年重点突破的技术方向：

### 1. 提示工程（写好文字=生成好图）
“提示词（Prompt）”是文字到图像的“翻译指令”，提示工程专注于“如何写提示词、或让模型自动优化提示词”：
- **PromptCharm**（CHI 2024）：自动优化粗糙提示词，比如把“一只猫”优化为“一只橘色短毛猫，趴在阳光照射的窗台，背景有绿萝，写实风格”，生成效果大幅提升。[论文](https://arxiv.org/abs/2403.04014)
- **自动化黑盒提示工程**（arXiv 2024）：针对“闭源模型（如Midjourney）”，自动测试不同提示词组合，找到最符合用户需求的版本（无需懂模型原理）。[论文](https://arxiv.org/abs/2403.191039)


### 2. 多模态控制（不止文字，还能加“约束”）
单一文字描述容易“失控”（比如想让“苹果在盘子左边”，结果生成在右边），多模态控制通过“文字+其他信号”精准约束生成：
- **布局控制**：输入“文字+布局图”（比如用方框标“盘子在中间，苹果在左上方”），模型严格按布局生成。代表：Zero-Painter（2024）、InstanceDiffusion（CVPR 2024，支持多物体分别控制）。
- **图像/视频约束**：输入“文字+参考图”，生成“符合文字描述、且保留参考图风格/结构”的图像。代表：MM-Diff（2024，高保真个性化）、ControlVideo（ICLR 2024，文本+参考图生成视频）。
- **跨模态生成**：支持“文本↔图像↔音频”互相转换，比如用文字生成图像后，再生成匹配的背景音。代表：4M-21（2024，支持6种模态）、CoDi（NeurIPS 2023）。


### 3. 效率与可控性优化
- **快速生成**：核心是“减少扩散模型的去噪步数”，代表是SDXL-Lightning（1-4步出图），满足实时创作需求。
- **细粒度控制**：针对“局部属性修改”，比如只换衣服颜色、不换人物姿态。代表：PreciseControl（面部属性）、Ctrl-X（2024，结构+外观分离控制）。


## 四、主流应用领域（能用来做什么？）
### 1. 内容创作与设计
- 平面设计：生成海报、LOGO草图（配合提示工程优化细节）；
- 艺术创作：生成油画、插画等风格化作品（如用Kandinsky 3指定“梵高风格的城市夜景”）；
- 角色/场景设计：游戏、动画的角色原型（如CosmicMan生成不同服饰的角色）、场景概念图。


### 2. 图像编辑与优化
- 生成填充：删除图像中多余物体（如“去掉照片里的垃圾桶”）、或补充缺失部分（如“给人物加上背景花海”），代表：Adobe Photoshop生成填充功能。
- 风格迁移：把一张图的风格迁移到另一张，比如“把我的自拍变成水墨画风”，代表：StyleShot（2024，快速捕捉参考图风格）。
- 超分辨率：用文字提示“把模糊的老照片变清晰，保留面部皱纹细节”，代表：文本提示超分辨率模型（2023）。


### 3. 垂直领域落地
- 3D行业：生成3D模型草图（如PlacidDreamer生成家具3D模型）；
- 文字相关：生成艺术化中文字符（GlyphDraw，2023）、文本角色插画（TextDiffuser，2023）；
- 影视/视频：生成长视频片段（Sora）、视频局部编辑（MagicStick，2023，用手柄控制视频中物体运动）。


## 五、实用工具与数据集（上手必备）
### 1. 开发工具
- **Hugging Face Diffusers**：最流行的T2I开发库，支持SD、Kandinsky等主流模型，提供开箱即用的生成/编辑API，适合Python开发者。[官网](https://huggingface.co/docs/diffusers/index)
- **ControlNet**：SD的控制插件，支持“线稿→图像”“深度图→图像”等，是可控生成的必备工具。[代码](https://github.com/lllyasviel/ControlNet)


### 2. 数据集
- **Pick-a-Pic**（2023）：全球最大的“用户偏好数据集”，包含百万级“文本+多张图像+用户选择结果”，可用来训练“更懂人类审美的模型”。[数据集](https://huggingface.co/datasets/yuvalkirstain/pickapic_v1)、[代码](https://github.com/yuvalkirstain/PickScore)


## 六、2024领域趋势总结
1.  **可控性为王**：从“生成图像”到“精准生成想要的图像”，细粒度属性控制（如面部、服饰）、多模态约束（如布局、参考图）成为核心方向；
2.  **多模态融合**：不再局限于“文本→图像”，而是“文本+图像+音频+3D”的跨模态生成与编辑（如Sora的文本→视频+音频）；
3.  **效率革命**：快速生成（如SDXL-Lightning）、轻量化模型（适配手机等端侧设备）成为落地关键；
4.  **开源与闭源并存**：闭源模型（Imagen 3、Sora）引领技术上限，开源模型（Kandinsky、SD系列）支撑社区创新与二次开发。
